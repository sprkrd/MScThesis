@article{martinez2017relational,
abstract = {Model-based reinforcement learning is a powerful paradigm for learning tasks in robotics. However, in-depth exploration is usually required and the actions have to be known in advance. Thus, we propose a novel algorithm that integrates the option of requesting teacher demonstrations to learn new domains with fewer action executions and no previous knowledge. Demonstrations allow new actions to be learned and they greatly reduce the amount of exploration required, but they are only requested when they are expected to yield a significant improvement because the teacher's time is considered to be more valuable than the robot's time. Moreover, selecting the appropriate action to demonstrate is not an easy task, and thus some guidance is provided to the teacher. The rule-based model is analyzed to determine the parts of the state that may be incomplete, and to provide the teacher with a set of possible problems for which a demonstration is needed. Rule analysis is also used to find better alternative models and to complete subgoals before requesting help, thereby minimizing the number of requested demonstrations. These improvements were demonstrated in a set of experiments, which included domains from the international planning competition and a robotic task. Adding teacher demonstrations and rule analysis reduced the amount of exploration required by up to 60{\%} in some domains, and improved the success ratio by 35{\%} in other domains.},
author = {Mart{\'{i}}nez, David and Aleny{\`{a}}, Guillem and Torras, Carme},
doi = {10.1016/j.artint.2015.02.006},
file = {:home/asuarez/Documents/MendeleyDesktop/Mart{\'{i}}nez, Aleny{\`{a}}, Torras/2017{\_}Relational reinforcement learning with guided demonstrations.pdf:pdf},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Active learning,Learning guidance,Planning excuse,Reinforcement learning,Robot learning,Teacher demonstration,Teacher guidance,learning,learning domains,reinforcement learning},
mendeley-tags = {learning,learning domains,reinforcement learning},
month = {jun},
pages = {295--312},
title = {{Relational reinforcement learning with guided demonstrations}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0004370215000284},
volume = {247},
year = {2017}
}

@article{yoon2010probabilistic,
	abstract = {Recently, â€˜determinization in hindsight' has enjoyed surprising success in on-line probabilistic planning. This technique evaluates the actions available in the current state by using non-probabilistic planning in deterministic approximations of the original domain. Although the approach has proven itself effective in many challenging domains, it is computationally very expensive. In this paper, we present three significant improvements to help mitigate this expense. First, we use a method for detecting potentially useful actions, allowing us to avoid estimating the values of unnecessary ones. Second, we exploit determinism in the domain by reusing relevant plans rather than computing new ones. Third, we improve action evaluation by increasing the chance that at least one determin- istic plan reaches a goal. Taken together, these improvements allowdeterminization in hindsight to scale significantly better on large or mostly-deterministic problems.},
	author = {Yoon, Sungwook and Ruml, Wheeler and Benton, J. and Do, Minh B.},
	file = {:home/asuarez/Documents/MendeleyDesktop/Yoon et al/2010{\_}Improving determinization in hindsight for online probabilistic planning.pdf:pdf},
	isbn = {9781577354499},
	journal = {20th International Conference on Automated Planning and Scheduling},
	keywords = {"probabilistic planning,optimization in hindsight},
	mendeley-groups = {sorted-by-theme/task planning/determinization,msc thesis},
	number = {ICAPS},
	pages = {209--216},
	title = {{Improving determinization in hindsight for online probabilistic planning}},
	url = {http://www.aaai.org/ocs/index.php/ICAPS/ICAPS10/paper/viewFile/1438/1561},
	year = {2010}
}


